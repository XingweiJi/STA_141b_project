{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# NLP and Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Background\n",
    "\n",
    "We have a data set called review which has the information of the review section. In this project, we will use review text, the star that the user give the business, how many people think this review is useful and business ID to perform NLP and machine learning.\n",
    "\n",
    "Below is a sample of review text that the user gave to the restaurant:\n",
    "\n",
    "```python\n",
    "test['text'][0] \n",
    "```\n",
    "\n",
    ">\"The pizza was okay. Not the best I've had. I prefer Biaggio's on Flamingo / Fort Apache. The chef there can make a MUCH better NY style pizza. The pizzeria @ Cosmo was over priced for the quality and lack of personality in the food. Biaggio's is a much better pick if youre going for italian - family owned, home made recipes, people that actually CARE if you like their food. You dont get that at a pizzeria in a casino. I dont care what you say...\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## NLP\n",
    "(This part corresponds to Machine Learning NLP.ipynb)\n",
    "\n",
    "**We will perform the sentiment extraction on selected review text. In our case, we choose reviews that have more than 8 'useful' upvotes which we believe is a good review.**    \n",
    "\n",
    "\n",
    "\n",
    "### 1. The importance of sentiment analysis\n",
    ">It's important to use sentiments to evaluate the review of business. Although 'stars' seems to be a sufficient estimator, it lacks objectivity. A person can give different stars under his/her certain mood even though the actual quality of the business is constant. Thus, bringing up sentiment into account is a good way of measuring the actually quality of the business"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Model selection\n",
    "\n",
    "**We have 3 models to be experimented on: TextBlob, SpaCy Text Categorizer and Google NLP api:**               \n",
    "1. TextBlob:   \n",
    "Textblob doesn't have a acceptable accuracy especially for food review. Naivebayes is slow and inaccurate. I did a little research on how textblob calculates the sentiment. It turns out they have a XML file that contains polarity score for each words, and the overall polarity score is just the average of polarity scores of each word (Link: https://planspace.org/20150607-textblob_sentiment/ ). This is a poor way of estimating the sentiment score. Consider training my own NLP model.                                                \n",
    "2. SpaCy Text Categorizer:  \n",
    "SpaCy is a package that allows developer to build their own NLP model. The base model they provide is CNN.        \n",
    "We use Amazon's food review dataset to train the model and the result is following:\n",
    "         Warning: Unnamed vectors -- this won't allow multiple vectors models to be loaded. (Shape: (0, 0))\n",
    "         Training the model...\n",
    "         LOSS \t  P  \t  R  \t  F  \n",
    "         204.768\t0.910\t0.983\t0.945\n",
    "         time for one iteration is 155.03080892562866\n",
    "         123.836\t0.925\t0.975\t0.949\n",
    "         time for one iteration is 178.083074092865\n",
    "         94.492\t0.930\t0.973\t0.951\n",
    "         time for one iteration is 176.36578583717346\n",
    "         84.068\t0.934\t0.969\t0.951\n",
    "         time for one iteration is 181.42677283287048\n",
    "         74.010\t0.934\t0.969\t0.951\n",
    "         time for one iteration is 185.3597228527069\n",
    "         68.926\t0.933\t0.969\t0.950\n",
    "         time for one iteration is 188.23954820632935\n",
    "         64.172\t0.935\t0.966\t0.950\n",
    "         time for one iteration is 177.8659210205078\n",
    "         60.054\t0.935\t0.968\t0.951\n",
    "         time for one iteration is 180.7558081150055\n",
    "         63.498\t0.935\t0.968\t0.951\n",
    "         time for one iteration is 178.3223419189453\n",
    "         57.024\t0.936\t0.968\t0.951\n",
    "         time for one iteration is 174.05425381660461\n",
    "         CPU times: user 48min 14s, sys: 1min 54s, total: 50min 9s\n",
    "         Wall time: 29min 35s                  \n",
    "The trained model doesn't perform well as well. The training takes 29 mins for just 10000 samples.         \n",
    "3. Google NLP api:               \n",
    "Google's API performs with the best accuracy. However, it costs.            \n",
    "\n",
    "**Conclusion: Textblob is not accurate, SpaCy Text Categorizer is slow in terms of training, Google API is accurate but expensive. We will proceed with Google's API and Textblob since we don't have time for training the spacy model**\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Comparison between textblob and Google's NLP API\n",
    "\n",
    "We performed both textblob and Google's API on the text. And we plot the distribution of the sentiments:    \n",
    "<img src=\"textblb_vs_Google.png\">\n",
    "\n",
    "We can see that textblob is more concentrated on 0-0.25 range. Google API is more spread which reflects the real situation.           \n",
    "\n",
    "\n",
    "\n",
    "**Conclusion: We finally decide to choose Google's api because we have $300 credit for each of us. Since we will use this information for furthur training. We need the most accurate model.**\n",
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Sentiment analysis\n",
    "\n",
    "* **Question: What's the relationship between review star and sentiments?**\n",
    "<img src= \"review_starsvsse.png\">\n",
    "It's not clear how they are related. Ideally, we expect high sentiment with how review score. However, this is not the case here.\n",
    "\n",
    "* **Questions: why there are some reviews having high stars but with low sentiments and vice versa?**         \n",
    "Check the review that has difference of stars and sentiments bigger than 1.5  \n",
    "\n",
    "```python\n",
    "df[abs(a - df['sentiments']) > 1.5].reset_index().loc[1,'text']\n",
    "```\n",
    ">the vacuums suck, that's for sure\\nsnagged a twenty, inside door\\npocket, swept it, straight away\\ncest le vie, most would say\\nbut I asked, \"could it be found?\"\\n\"sure, we'll shut, the vacuums down\"\\nowners, no less, had to go\\nto a place, only, Mike Rowe\\nof Dirty Jobs, would lurk around\\nthe filth, from every car in town\\ncollecting, in some giant vat\\nwhile they looked, there I sat\\nadmiring, this great idea\\nand how, ideas, are crystal clear\\nto some, but not to others\\nbuild a better mouse trap, brother\\nwhich they have, at Clean Freak\\nget your car washed, on the cheap\\npseudo self-serve, shoot the tube\\nfelt, just like, some surfer dude\\nin a blue wave, closing out\\nsoap and suds, sloshed about\\nspot free rinse, turtle wax\\nspit out, spotless, front to back\\nya, I bought, the monthly pass\\nbefore, my car, it looked liked ass\\nfilthy, crusty, dusty, dirty \\nah shucks, now, she sure looks purdy\\n'course Clean Freak's got, karma's attraction\\nfound, returned, my andrew jackson'\n",
    "\n",
    "```python\n",
    "df[abs(a - df['sentiments']) > 1.5].reset_index().loc[1,['review_stars','sentiments']]\n",
    "```\n",
    ">rview_stars      5       \n",
    "sentiments     -0.7         \n",
    "Name: 1, dtype: object           \n",
    "\n",
    "\n",
    "As you can see, some reviews are clearly negative, yet have a 5 star rating and vice versa. These negativities are captured by sentiments and should be considered \"dishonest review\" which can be detected by the code above."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Machine Learning\n",
    "(This corresponds to Machine learning.ipynb)\n",
    "\n",
    "### Goal\n",
    "We want to predict the star that user is going to give the restaurant based on the review's sentiments, checkin counts and restaurant's information\n",
    "\n",
    "### 1. Data preprocessing\n",
    "* Drop reviews that are not honest(difference of sentiments and star is bigger than 1.5)\n",
    "* turn nun-numerical data into string for encoding later\n",
    "* drop NA values in sentiments\n",
    "* encode the categorical variable to foat arrays using onehotencoder\n",
    "* split traning and testing set\n",
    "\n",
    "### 2. Neural network\n",
    "#### Architecture\n",
    "The neural network we choose has 4 layers total:      \n",
    "1st layer has 985 nodes with the input dimension of 984 (one node for bias)           \n",
    "2nd and 3rd layers are hidden layers with 3 nodes (converge fast)           \n",
    "4th layer is output layer with 5 nodes       \n",
    "#### Training\n",
    "We train the model with training dataset. The optimizer is sgd(stochastic gradient descent) and loss is defined as categorical_crossentropy.\n",
    "```python\n",
    "sgd = keras.optimizers.SGD(lr=0.1, momentum=0.0, decay=0.0, nesterov=False)\n",
    "model.compile(optimizer=sgd,\n",
    "              loss='categorical_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "model.fit(x_train,y_train,epochs=100)\n",
    "```\n",
    "#### Result\n",
    "The model ends up with a loss of 0.8438 and a testing accuracy of 0.6278. Due to limited computation power, it hasn't converged yet. Here is the traning curve where x axis is epochs and y axis is accuracy\n",
    "<img src= 'nn.png'>\n",
    "\n",
    "\n",
    "### 3. Logistic regression\n",
    "The logistic model use l1 penalty and it has a 0.613910530283991 accuracy\n",
    "```python\n",
    "logi = LogisticRegression(penalty='l1').fit(x_train,y_train_logis)\n",
    "```\n",
    "### 4. SVM with cross validation\n",
    "We use 10 folds cross validation with SVM, then draw the ROC curve to mensure the performance.\n",
    "Due to computation limitation, we can only perform this algorithm on 5000 samples. \n",
    "Since we have multiclasses, we decide to compute `micro-average` ROC curve and AUC. We also compared the performance with and without feature selection.      \n",
    "\n",
    "* SVM CV with feature selection (logistic regression with l1 penalty selected features)\n",
    "```python\n",
    "svm_with_cv(X, Y, 'review_stars', True)\n",
    "```\n",
    "<img src='roc1.png'>\n",
    "<img src='pr1.png'>\n",
    "\n",
    "* SVM CV without feature selection (logistic regression with l1 penalty selected features)\n",
    "```python\n",
    "svm_with_cv(X, Y, 'review_stars', False)\n",
    "```\n",
    "<img src='roc2.png'>\n",
    "<img src='pr2.png'>\n",
    "\n",
    "**Conclusion: We can see that the ROC with feature selection has AUC of 0.81 and AURC(area under PR curve) of 0.56. And the ROC without feature selection has AUC of 0.81 abd AURC of 0.54. We can safely conclude that feature selection works very well.**\n",
    "### 5. SVD dimension reduction\n",
    "We also try to reduce the dimension of input. We use SVD method with components=20.   \n",
    "```python\n",
    "svd = TruncatedSVD(n_components=20, random_state=42)\n",
    "reduced_X = svd.fit_transform(X)\n",
    "svm_with_cv(reduced_X, Y, 'review_stars', False)\n",
    "```\n",
    "The ROC curves are follows:   \n",
    "<img src='rocd.png'>\n",
    "<img src='prd.png'>\n",
    "\n",
    "**Conclusion: SVD dimension reduction doesn't maintain or improve model accuracy.**\n",
    "### 6. Conclusion\n",
    "We tried different models and each have their pros and cons, Nueral Net is more accurate and adjustable. We can do grid search to tune the hyperparameter in the future. Logistic regression is fast yet less accurate. SVM is accurate but takes a long time to train. We can also explore more dimension reduction methods in the future. For now, we decide to use trained model from neural network to do the application."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Given an user's review and business's information, we can predict the star that the user is going to give. However, some users will give inaccurate stars which don't suit their reviews. Then our algorithms can alert them so that it doesn't affect the overall review of the business.       \n",
    "Here is an example:\n",
    "<img src='os1.png'>\n",
    "<img src='os2.png'>\n",
    "We can see that before our prediction, there are more 2 and 3 stars reviews. After prediction, we have less 2 and 3 stars reviews. We suspect that users always tend to give lower scores if they are not satisfied even if the actual quality is not as bad as they think especillay for 2 or 3 stars range."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
